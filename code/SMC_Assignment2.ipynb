{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raqOcwVMXnoC"
      },
      "outputs": [],
      "source": [
        "#Variables that contains the user credentials to access Twitter API\n",
        "access_token = \"1334322229983629313-ymXao8muISYUc5qz7R287bMODavZZY\"\n",
        "access_token_secret = \"uhfhsVcvF80K2aQC3Ah8wHH8mnPPS2bk7ckPrShS2ilzn\"\n",
        "consumer_key = \"LdwgPnL1Ay1TQfwuyCJRQL5mk\"\n",
        "consumer_secret = \"CsTmvEIcA7Xd4Fl0WIcqTbapLqBkMC9EaZjXvx8Ps9LCfG7qZU\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thmZz0qBbsSY",
        "outputId": "3c7ea454-fd87-45bb-c617-1a2ec72c01a0"
      },
      "outputs": [],
      "source": [
        "import tweepy\n",
        "from tweepy import OAuthHandler\n",
        "import csv\n",
        "import pandas as pd\n",
        "import re \n",
        "import tweepy \n",
        "import nltk\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "nltk.download('punkt')   \n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from textblob import TextBlob\n",
        "import emoji\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "#File to write if you would like to have better observation\n",
        "\n",
        "\n",
        "auth = OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hl-omtm8b1tN",
        "outputId": "eae68d63-3db9-4fe5-d80f-0861476d26c0"
      },
      "outputs": [],
      "source": [
        "#code to collect the target brand tweets\n",
        "counter = 0\n",
        "\n",
        "with open('samsung_data.csv', 'a') as f:\n",
        "  writer = csv.writer(f)\n",
        "  writer.writerow(['Author', 'Date', 'Text'])\n",
        "\n",
        "for tweet in tweepy.Cursor(api.search_tweets, q=('samsung AND (mobile OR phone)'), lang=\"en\", until='2021-10-26',tweet_mode='extended').items(30000): \n",
        "\n",
        "  print (\"Tweet created:\", tweet.created_at)\n",
        "  print (\"Tweet:\", tweet.full_text)\n",
        "\n",
        "  with open('samsung_data.csv', 'a', encoding=\"utf-8\") as f:\n",
        "      writer = csv.writer(f)\n",
        "      writer.writerow([tweet.user.name, tweet.created_at, tweet.full_text])\n",
        "\n",
        "  counter += 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hl-omtm8b1tN",
        "outputId": "eae68d63-3db9-4fe5-d80f-0861476d26c0"
      },
      "outputs": [],
      "source": [
        "#code to collect the target brand tweets\n",
        "#xiaomi\n",
        "counter = 0\n",
        "\n",
        "with open('xiaomi_data.csv', 'a') as f:\n",
        "  writer = csv.writer(f)\n",
        "  writer.writerow(['Author', 'Date', 'Text'])\n",
        "\n",
        "for tweet in tweepy.Cursor(api.search_tweets, q=('(xiaomi OR redmi) AND (mobile OR phone)'), lang=\"en\", until='2021-10-26',tweet_mode='extended').items(20000): \n",
        "  print (\"Tweet created:\", tweet.created_at)\n",
        "  print (\"Tweet:\", tweet.full_text)\n",
        "\n",
        "  with open('xiaomi_data.csv', 'a', encoding=\"utf-8\") as f:\n",
        "      writer = csv.writer(f)\n",
        "      writer.writerow([tweet.user.name, tweet.created_at, tweet.full_text])\n",
        "\n",
        "  counter += 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hl-omtm8b1tN",
        "outputId": "eae68d63-3db9-4fe5-d80f-0861476d26c0"
      },
      "outputs": [],
      "source": [
        "#code to collect the target brand tweets\n",
        "#huawei\n",
        "counter = 0\n",
        "\n",
        "with open('huawei_data.csv', 'a') as f:\n",
        "  writer = csv.writer(f)\n",
        "  writer.writerow(['Author', 'Date', 'Text'])\n",
        "\n",
        "for tweet in tweepy.Cursor(api.search_tweets, q=('huawei AND (mobile OR phone)'), lang=\"en\", until='2021-10-26',tweet_mode='extended').items(30000): \n",
        "\n",
        "  print (\"Tweet created:\", tweet.created_at)\n",
        "  print (\"Tweet:\", tweet.full_text)\n",
        "\n",
        "  with open('huawei_data.csv', 'a', encoding=\"utf-8\") as f:\n",
        "      writer = csv.writer(f)\n",
        "      writer.writerow([tweet.user.name, tweet.created_at, tweet.full_text])\n",
        "\n",
        "  counter += 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hl-omtm8b1tN",
        "outputId": "eae68d63-3db9-4fe5-d80f-0861476d26c0"
      },
      "outputs": [],
      "source": [
        "#code to collect the target brand tweets\n",
        "#oppo\n",
        "counter = 0\n",
        "\n",
        "with open('oppo_data.csv', 'a') as f:\n",
        "  writer = csv.writer(f)\n",
        "  writer.writerow(['Author', 'Date', 'Text'])\n",
        "\n",
        "for tweet in tweepy.Cursor(api.search_tweets, q=('oppo AND (mobile OR phone)'), lang=\"en\", until='2021-10-26',tweet_mode='extended').items(30000): \n",
        "\n",
        "  print (\"Tweet created:\", tweet.created_at)\n",
        "  print (\"Tweet:\", tweet.full_text)\n",
        "\n",
        "  with open('oppo_data.csv', 'a', encoding=\"utf-8\") as f:\n",
        "      writer = csv.writer(f)\n",
        "      writer.writerow([tweet.user.name, tweet.created_at, tweet.full_text])\n",
        "\n",
        "  counter += 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hl-omtm8b1tN",
        "outputId": "eae68d63-3db9-4fe5-d80f-0861476d26c0"
      },
      "outputs": [],
      "source": [
        "#code to collect the target brand tweets\n",
        "#apple iphone\n",
        "counter = 0\n",
        "\n",
        "with open('apple_data.csv', 'a') as f:\n",
        "  writer = csv.writer(f)\n",
        "  writer.writerow(['Author', 'Date', 'Text'])\n",
        "\n",
        "for tweet in tweepy.Cursor(api.search_tweets, q=('(apple OR iphone) AND (mobile OR phone)'), lang=\"en\", until='2021-10-26',tweet_mode='extended').items(30000): \n",
        "\n",
        "  print (\"Tweet created:\", tweet.created_at)\n",
        "  print (\"Tweet:\", tweet.full_text)\n",
        "\n",
        "  with open('apple_data.csv', 'a', encoding=\"utf-8\") as f:\n",
        "      writer = csv.writer(f)\n",
        "      writer.writerow([tweet.user.name, tweet.created_at, tweet.full_text])\n",
        "\n",
        "  counter += 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BNDNbCXFtCJo",
        "outputId": "3f13e156-9cb0-406f-dd65-38de4885b852"
      },
      "outputs": [],
      "source": [
        "#code to collect different brands data in the industry\n",
        "with open('industry_data.csv', 'a') as f:\n",
        "  writer = csv.writer(f)\n",
        "  writer.writerow(['Author', 'Date', 'Text'])\n",
        "\n",
        "for tweet in tweepy.Cursor(api.search_tweets, q=('(mobile OR phone) AND (xiaomi OR samsung OR huawei OR oppo OR apple) '), since='2021-08-20', lang=\"en\", until='2021-10-20',tweet_mode='extended').items(50000): \n",
        "\n",
        "  #print (\"Tweet created:\", tweet.created_at)\n",
        "  #print (\"Tweet:\", tweet.text)\n",
        "  try:\n",
        "\n",
        "    with open('industry_data.csv', 'a',encoding=\"utf-8\") as f:\n",
        "      print(\"Tweet:\", tweet.full_text)\n",
        "      writer = csv.writer(f)\n",
        "      writer.writerow([tweet.user.name, tweet.created_at, tweet.full_text])\n",
        "\n",
        "\n",
        "  except exception as e:\n",
        "    pass \n",
        "                                "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uO0CRn_2bwI4",
        "outputId": "6ce26686-0864-4828-fcdc-3c4c03dda1b2"
      },
      "outputs": [],
      "source": [
        "samsung_data = pd.read_csv(\"samsung_data.csv\")\n",
        "apple_data = pd.read_csv(\"apple_data.csv\")\n",
        "oppo_data = pd.read_csv(\"oppo_data.csv\")\n",
        "huawei_data = pd.read_csv(\"huawei_data.csv\")\n",
        "xiaomi_data = pd.read_csv(\"xiaomi_data.csv\")\n",
        "#industry_data = pd.read_csv(\"industry_data.csv\")\n",
        "\n",
        "samsung_tweets = []\n",
        "apple_tweets = []\n",
        "oppo_tweets = []\n",
        "xiaomi_tweets = []\n",
        "huawei_tweets = []\n",
        "#industry_tweets = []\n",
        "\n",
        "samsung_tweets = samsung_data[\"Text\"].tolist()\n",
        "apple_tweets = apple_data[\"Text\"].tolist()\n",
        "oppo_tweets = oppo_data[\"Text\"].tolist()\n",
        "huawei_tweets = huawei_data[\"Text\"].tolist()\n",
        "xiaomi_tweets = xiaomi_data[\"Text\"].tolist()\n",
        "#industry_tweets = industry_data[\"Text\"].tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHPsHYqeKZTN"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "  #lower case the text\n",
        "  text = text.lower()\n",
        "  # Removes all mentions (@username) from the tweet since it is of no use to us\n",
        "  text = re.sub(r'(@[A-Za-z0-9_]+)', ' ', text)\n",
        "    \n",
        "  # Removes any link in the text\n",
        "  text = re.sub('http://\\S+|https://\\S+', ' ', text)\n",
        "\n",
        "  # Removes the word \"rt\" \n",
        "  text = re.sub(r'\\brt\\b', ' ', text).strip()\n",
        "  \n",
        "   # Removes the word \"mobile\" and 'phone' and brands name cause it appears too many times that might affect topic modelling\n",
        "   \n",
        "  text = re.sub(r'\\bmobile\\b', ' ', text).strip()\n",
        "  text = re.sub(r'\\bphone\\b', ' ', text).strip()\n",
        "  text = re.sub(r'\\biphone\\b', ' ', text).strip()\n",
        "  text = re.sub(r'\\bapple\\b', ' ', text).strip()\n",
        "  text = re.sub(r'\\bsamsung\\b', ' ', text).strip()\n",
        "  text = re.sub(r'\\boppo\\b', ' ', text).strip()\n",
        "  text = re.sub(r'\\bxiaomi\\b', ' ', text).strip()\n",
        "  text = re.sub(r'\\bhuawei\\b', ' ', text).strip()\n",
        "  text = re.sub(r'\\bredmi\\b', ' ', text).strip()\n",
        "\n",
        "  # Only considers the part of the string with char between a to z or digits and whitespace characters\n",
        "  # Basically removes punctuation\n",
        "  text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "\n",
        "  #remove number\n",
        "  text = ''.join([i for i in text if not i.isdigit()])\n",
        "\n",
        "  #remove emoji\n",
        "  text = emoji.demojize(text, language ='en')\n",
        "\n",
        "  # Removes stop words that have no use in sentiment analysis \n",
        "  text_tokens = word_tokenize(text)\n",
        "  text = [word for word in text_tokens if not word in stopwords.words()]\n",
        "\n",
        "  text = ' '.join(text)\n",
        "  return text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mobDTVRLRV-",
        "outputId": "e8b6c734-0924-41ae-dea2-7a79ca31867f"
      },
      "outputs": [],
      "source": [
        "cleaned_samsung_text = []\n",
        "\n",
        "for tweets in samsung_tweets:\n",
        "  cleaned_samsung_text.append(clean_text(tweets))\n",
        "\n",
        "print(cleaned_samsung_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S37FyFBZSDSu",
        "outputId": "2aabcab6-938e-428f-925c-84ef132314b8"
      },
      "outputs": [],
      "source": [
        "cleaned_apple_text = []\n",
        "\n",
        "for tweets in apple_tweets:\n",
        "  cleaned_apple_text.append(clean_text(tweets))\n",
        "\n",
        "print(cleaned_apple_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cleaned_oppo_text = []\n",
        "\n",
        "for tweets in oppo_tweets:\n",
        "  cleaned_oppo_text.append(clean_text(tweets))\n",
        "\n",
        "print(cleaned_oppo_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cleaned_xiaomi_text = []\n",
        "\n",
        "for tweets in xiaomi_tweets:\n",
        "  cleaned_xiaomi_text.append(clean_text(tweets))\n",
        "\n",
        "print(cleaned_xiaomi_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cleaned_huawei_text = []\n",
        "\n",
        "for tweets in huawei_tweets:\n",
        "  cleaned_huawei_text.append(clean_text(tweets))\n",
        "\n",
        "print(cleaned_huawei_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sRxafkp_9-N"
      },
      "outputs": [],
      "source": [
        "cleaned_samsung_text = list(dict.fromkeys(cleaned_samsung_text))\n",
        "with open('cleaned_samsung.txt', 'w',encoding=\"utf-8\") as f:\n",
        "    for item in cleaned_samsung_text:\n",
        "        f.write(\"%s\\n\" % item)\n",
        "        \n",
        "cleaned_apple_text = list(dict.fromkeys(cleaned_apple_text))\n",
        "with open('cleaned_apple.txt', 'w',encoding=\"utf-8\") as f:\n",
        "    for item in cleaned_apple_text:\n",
        "        f.write(\"%s\\n\" % item)\n",
        "        \n",
        "cleaned_xiaomi_text = list(dict.fromkeys(cleaned_xiaomi_text))\n",
        "with open('cleaned_xiaomi.txt', 'w',encoding=\"utf-8\") as f:\n",
        "    for item in cleaned_xiaomi_text:\n",
        "        f.write(\"%s\\n\" % item)\n",
        "        \n",
        "cleaned_huawei_text = list(dict.fromkeys(cleaned_huawei_text))\n",
        "with open('cleaned_huawei.txt', 'w',encoding=\"utf-8\") as f:\n",
        "    for item in cleaned_huawei_text:\n",
        "        f.write(\"%s\\n\" % item)\n",
        "        \n",
        "cleaned_oppo_text = list(dict.fromkeys(cleaned_oppo_text))\n",
        "with open('cleaned_oppo.txt', 'w',encoding=\"utf-8\") as f:\n",
        "    for item in cleaned_oppo_text:\n",
        "        f.write(\"%s\\n\" % item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ea8Ukin8eQav"
      },
      "outputs": [],
      "source": [
        "cleaned_industry_text = list(dict.fromkeys(cleaned_industry_text))\n",
        "with open('cleaned_industry.txt', 'w',encoding=\"utf-8\") as f:\n",
        "    for item in cleaned_industry_text:\n",
        "        f.write(\"%s\\n\" % item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nTZ0YKsK9lH2",
        "outputId": "cbbad382-a5c3-47b5-ec21-61d5f199aa79"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    Count_neg=0\n",
        "    Count_neutral=0\n",
        "    Count_pos=0\n",
        "\n",
        "    inputfile = open(\"cleaned_samsung.txt\", \"r\",encoding=\"utf-8\") #create a txt file and insert a few sentences\n",
        "    \n",
        "\n",
        "    for line in inputfile:\n",
        "        try:\n",
        "            print(line)\n",
        "            line = TextBlob(line)\n",
        "\n",
        "           \n",
        "            print (\"Input sentence = \", line.strip())            \n",
        "            print(line.sentiment)\n",
        "            \n",
        "\n",
        "            if line.sentiment.polarity < 0.0:\n",
        "                print (\"negative\" )                \n",
        "                Count_neg +=1\n",
        "            elif line.sentiment.polarity == 0:\n",
        "                print (\"neutral\")                \n",
        "                Count_neutral +=1\n",
        "            else:\n",
        "                print (\"positive\")                \n",
        "                Count_pos +=1\n",
        "                \n",
        "            print(\"==============================================================\")\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    print (\"Count Negative = \", Count_neg)\n",
        "    print (\"Count Neutral = \", Count_neutral)\n",
        "    print (\"Count Positive = \", Count_pos)\n",
        "    \n",
        "\n",
        "    Polarity = [1,2,3]\n",
        "    LABELS = [\"Positive\", \"Neutral\", \"Negative\"]\n",
        "    Count_polarity = [Count_pos, Count_neutral, Count_neg]\n",
        "\n",
        "    plt.xlabel('Polarity')\n",
        "    plt.ylabel('Count')\n",
        "    plt.title('Samsung Sentiment Analysis')\n",
        "\n",
        "    plt.grid(True)\n",
        "    plt.bar(Polarity, Count_polarity, align='center')\n",
        "    plt.xticks(Polarity, LABELS)\n",
        "    plt.savefig('samsung_sentiment'+\".png\", bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    Count_neg=0\n",
        "    Count_neutral=0\n",
        "    Count_pos=0\n",
        "\n",
        "    inputfile = open(\"cleaned_apple.txt\", \"r\",encoding=\"utf-8\") #create a txt file and insert a few sentences\n",
        "    \n",
        "\n",
        "    for line in inputfile:\n",
        "        try:\n",
        "            print(line)\n",
        "            line = TextBlob(line)\n",
        "\n",
        "           \n",
        "            print (\"Input sentence = \", line.strip())            \n",
        "            print(line.sentiment)\n",
        "            \n",
        "\n",
        "            if line.sentiment.polarity < 0.0:\n",
        "                print (\"negative\" )                \n",
        "                Count_neg +=1\n",
        "            elif line.sentiment.polarity == 0:\n",
        "                print (\"neutral\")                \n",
        "                Count_neutral +=1\n",
        "            else:\n",
        "                print (\"positive\")                \n",
        "                Count_pos +=1\n",
        "                \n",
        "            print(\"==============================================================\")\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    print (\"Count Negative = \", Count_neg)\n",
        "    print (\"Count Neutral = \", Count_neutral)\n",
        "    print (\"Count Positive = \", Count_pos)\n",
        "    \n",
        "\n",
        "    Polarity = [1,2,3]\n",
        "    LABELS = [\"Positive\", \"Neutral\", \"Negative\"]\n",
        "    Count_polarity = [Count_pos, Count_neutral, Count_neg]\n",
        "\n",
        "    plt.xlabel('Polarity')\n",
        "    plt.ylabel('Count')\n",
        "    plt.title('Apple Sentiment Analysis')\n",
        "\n",
        "    plt.grid(True)\n",
        "    plt.bar(Polarity, Count_polarity, align='center')\n",
        "    plt.xticks(Polarity, LABELS)\n",
        "    plt.savefig('apple_sentiment'+\".png\", bbox_inches='tight')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XQRFUJMUenOY",
        "outputId": "432198dc-fc9b-48fc-fa03-eeb14ac1f4f8"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    Count_neg=0\n",
        "    Count_neutral=0\n",
        "    Count_pos=0\n",
        "\n",
        "    inputfile = open(\"cleaned_oppo.txt\", \"r\",encoding=\"utf-8\") #create a txt file and insert a few sentences\n",
        "    \n",
        "\n",
        "    for line in inputfile:\n",
        "        try:\n",
        "            print(line)\n",
        "            line = TextBlob(line)\n",
        "\n",
        "           \n",
        "            print (\"Input sentence = \", line.strip())            \n",
        "            print(line.sentiment)\n",
        "            \n",
        "\n",
        "            if line.sentiment.polarity < 0.0:\n",
        "                print (\"negative\" )                \n",
        "                Count_neg +=1\n",
        "            elif line.sentiment.polarity == 0:\n",
        "                print (\"neutral\")                \n",
        "                Count_neutral +=1\n",
        "            else:\n",
        "                print (\"positive\")                \n",
        "                Count_pos +=1\n",
        "                \n",
        "            print(\"==============================================================\")\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    print (\"Count Negative = \", Count_neg)\n",
        "    print (\"Count Neutral = \", Count_neutral)\n",
        "    print (\"Count Positive = \", Count_pos)\n",
        "    \n",
        "\n",
        "    Polarity = [1,2,3]\n",
        "    LABELS = [\"Positive\", \"Neutral\", \"Negative\"]\n",
        "    Count_polarity = [Count_pos, Count_neutral, Count_neg]\n",
        "\n",
        "    plt.xlabel('Polarity')\n",
        "    plt.ylabel('Count')\n",
        "    plt.title('Oppo Sentiment Analysis')\n",
        "\n",
        "    plt.grid(True)\n",
        "    plt.bar(Polarity, Count_polarity, align='center')\n",
        "    plt.xticks(Polarity, LABELS)\n",
        "    plt.savefig('oppo_sentiment'+\".png\", bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    Count_neg=0\n",
        "    Count_neutral=0\n",
        "    Count_pos=0\n",
        "\n",
        "    inputfile = open(\"cleaned_huawei.txt\", \"r\",encoding=\"utf-8\") #create a txt file and insert a few sentences\n",
        "    \n",
        "\n",
        "    for line in inputfile:\n",
        "        try:\n",
        "            print(line)\n",
        "            line = TextBlob(line)\n",
        "\n",
        "           \n",
        "            print (\"Input sentence = \", line.strip())            \n",
        "            print(line.sentiment)\n",
        "            \n",
        "\n",
        "            if line.sentiment.polarity < 0.0:\n",
        "                print (\"negative\" )                \n",
        "                Count_neg +=1\n",
        "            elif line.sentiment.polarity == 0:\n",
        "                print (\"neutral\")                \n",
        "                Count_neutral +=1\n",
        "            else:\n",
        "                print (\"positive\")                \n",
        "                Count_pos +=1\n",
        "                \n",
        "            print(\"==============================================================\")\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    print (\"Count Negative = \", Count_neg)\n",
        "    print (\"Count Neutral = \", Count_neutral)\n",
        "    print (\"Count Positive = \", Count_pos)\n",
        "    \n",
        "\n",
        "    Polarity = [1,2,3]\n",
        "    LABELS = [\"Positive\", \"Neutral\", \"Negative\"]\n",
        "    Count_polarity = [Count_pos, Count_neutral, Count_neg]\n",
        "\n",
        "    plt.xlabel('Polarity')\n",
        "    plt.ylabel('Count')\n",
        "    plt.title('Huawei Sentiment Analysis')\n",
        "\n",
        "    plt.grid(True)\n",
        "    plt.bar(Polarity, Count_polarity, align='center')\n",
        "    plt.xticks(Polarity, LABELS)\n",
        "    plt.savefig('huawei_sentiment'+\".png\", bbox_inches='tight')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    Count_neg=0\n",
        "    Count_neutral=0\n",
        "    Count_pos=0\n",
        "\n",
        "    inputfile = open(\"cleaned_xiaomi.txt\", \"r\",encoding=\"utf-8\") #create a txt file and insert a few sentences\n",
        "    \n",
        "\n",
        "    for line in inputfile:\n",
        "        try:\n",
        "            print(line)\n",
        "            line = TextBlob(line)\n",
        "\n",
        "           \n",
        "            print (\"Input sentence = \", line.strip())            \n",
        "            print(line.sentiment)\n",
        "            \n",
        "\n",
        "            if line.sentiment.polarity < 0.0:\n",
        "                print (\"negative\" )                \n",
        "                Count_neg +=1\n",
        "            elif line.sentiment.polarity == 0:\n",
        "                print (\"neutral\")                \n",
        "                Count_neutral +=1\n",
        "            else:\n",
        "                print (\"positive\")                \n",
        "                Count_pos +=1\n",
        "                \n",
        "            print(\"==============================================================\")\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    print (\"Count Negative = \", Count_neg)\n",
        "    print (\"Count Neutral = \", Count_neutral)\n",
        "    print (\"Count Positive = \", Count_pos)\n",
        "    \n",
        "\n",
        "    Polarity = [1,2,3]\n",
        "    LABELS = [\"Positive\", \"Neutral\", \"Negative\"]\n",
        "    Count_polarity = [Count_pos, Count_neutral, Count_neg]\n",
        "\n",
        "    plt.xlabel('Polarity')\n",
        "    plt.ylabel('Count')\n",
        "    plt.title('Xiaomi Sentiment Analysis')\n",
        "\n",
        "    plt.grid(True)\n",
        "    plt.bar(Polarity, Count_polarity, align='center')\n",
        "    plt.xticks(Polarity, LABELS)\n",
        "    plt.savefig('xiaomi_sentiment'+\".png\", bbox_inches='tight')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2S3yYLfiudS"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# spacy for lemmatization\n",
        "import spacy\n",
        "\n",
        "# Plotting tools\n",
        "import pyLDAvis\n",
        "#import pyLDAvis.gensim_models  # don't skip this\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Tknuh0EjFI3"
      },
      "outputs": [],
      "source": [
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out\n",
        "\n",
        "#tokenize words and clean up text\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvfoDdCDw7Et",
        "outputId": "c3918261-4bbb-4374-b53a-2ccae6a2da44"
      },
      "outputs": [],
      "source": [
        "file = open(\"cleaned_samsung.txt\",\"r\",encoding=\"utf-8\")\n",
        "content = file.read()\n",
        "samsung_data = content.split(\"\\n\")\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file = open(\"cleaned_oppo.txt\",\"r\",encoding=\"utf-8\")\n",
        "content = file.read()\n",
        "oppo_data = content.split(\"\\n\")\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file = open(\"cleaned_xiaomi.txt\",\"r\",encoding=\"utf-8\")\n",
        "content = file.read()\n",
        "xiaomi_data = content.split(\"\\n\")\n",
        "file.close()\n",
        "\n",
        "temp=[]\n",
        "for s in xiaomi_data:\n",
        "    temp.append(clean_text(s))\n",
        "\n",
        "xiaomi_data = temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file = open(\"cleaned_apple.txt\",\"r\",encoding=\"utf-8\")\n",
        "content = file.read()\n",
        "apple_data = content.split(\"\\n\")\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file = open(\"cleaned_huawei.txt\",\"r\",encoding=\"utf-8\")\n",
        "content = file.read()\n",
        "huawei_data = content.split(\"\\n\")\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkY2TiJ3oIht",
        "outputId": "00100566-1ce7-43da-8155-ea92416532c9"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwaUoGdGld0j"
      },
      "outputs": [],
      "source": [
        "tokenized_samsung = list(map(str.split, samsung_data))\n",
        "tokenized_apple = list(map(str.split, apple_data))\n",
        "tokenized_oppo = list(map(str.split, oppo_data))\n",
        "tokenized_huawei = list(map(str.split, huawei_data))\n",
        "tokenized_xiaomi = list(map(str.split, xiaomi_data))\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        " \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "samsung_lemmatized = []\n",
        "apple_lemmatized = []\n",
        "oppo_lemmatized = []\n",
        "huawei_lemmatized = []\n",
        "xiaomi_lemmatized = []\n",
        "\n",
        "\n",
        "for s in tokenized_samsung:\n",
        "  temp_list = []\n",
        "  for x in s:\n",
        "    temp = lemmatizer.lemmatize(x)\n",
        "    temp_list.append(temp)\n",
        "  samsung_lemmatized.append(temp_list)\n",
        "  \n",
        "for s in tokenized_apple:\n",
        "  temp_list = []\n",
        "  for x in s:\n",
        "    temp = lemmatizer.lemmatize(x)\n",
        "    temp_list.append(temp)\n",
        "  apple_lemmatized.append(temp_list)\n",
        "  \n",
        "for s in tokenized_oppo:\n",
        "  temp_list = []\n",
        "  for x in s:\n",
        "    temp = lemmatizer.lemmatize(x)\n",
        "    temp_list.append(temp)\n",
        "  oppo_lemmatized.append(temp_list)\n",
        "  \n",
        "for s in tokenized_xiaomi:\n",
        "  temp_list = []\n",
        "  for x in s:\n",
        "    temp = lemmatizer.lemmatize(x)\n",
        "    temp_list.append(temp)\n",
        "  xiaomi_lemmatized.append(temp_list)\n",
        "  \n",
        "for s in tokenized_huawei:\n",
        "  temp_list = []\n",
        "  for x in s:\n",
        "    temp = lemmatizer.lemmatize(x)\n",
        "    temp_list.append(temp)\n",
        "  huawei_lemmatized.append(temp_list)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "industry_data = samsung_data + huawei_data + apple_data + oppo_data +xiaomi_data\n",
        "tokenized_industry = list(map(str.split, industry_data))\n",
        "industry_lemmatized = []\n",
        "\n",
        "for s in tokenized_industry:\n",
        "  temp_list = []\n",
        "  for x in s:\n",
        "    temp = lemmatizer.lemmatize(x)\n",
        "    temp_list.append(temp)\n",
        "  industry_lemmatized.append(temp_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(industry_lemmatized)\n",
        "\n",
        "# Create Corpus\n",
        "texts = industry_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "\n",
        "coherence = []\n",
        "for k in range(2,10):\n",
        "    print('Round: '+str(k))\n",
        "    Lda = gensim.models.ldamodel.LdaModel\n",
        "    ldamodel = Lda(corpus, num_topics=k, id2word = id2word, passes=20,\\\n",
        "                   iterations=20, chunksize = 100, eval_every = None, random_state=100)\n",
        "    \n",
        "    cm = gensim.models.coherencemodel.CoherenceModel(model=ldamodel, texts=texts,\\\n",
        "                                                     dictionary=id2word, coherence='c_v')\n",
        "    coherence.append((k,cm.get_coherence()))\n",
        "    \n",
        "x_val = [x[0] for x in coherence]\n",
        "y_val = [x[1] for x in coherence]\n",
        "\n",
        "\n",
        "plt.plot(x_val,y_val)\n",
        "plt.scatter(x_val,y_val)\n",
        "plt.title('Number of Topics vs. Coherence')\n",
        "plt.xlabel('Number of Topics')\n",
        "plt.ylabel('Coherence')\n",
        "plt.xticks(x_val)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "from matplotlib import pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.colors as mcolors\n",
        "#creating industry lda model\n",
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(industry_lemmatized)\n",
        "\n",
        "# Create Corpus\n",
        "texts = industry_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=5, \n",
        "                                           chunksize=100,\n",
        "                                           iterations=20,\n",
        "                                           passes=20,\n",
        "                                           eval_every = None,\n",
        "                                           random_state=100)\n",
        "\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]\n",
        "\n",
        "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=industry_lemmatized):\n",
        "    # Init output\n",
        "    sent_topics_df = pd.DataFrame()\n",
        "\n",
        "    # Get main topic in each document\n",
        "    for i, row_list in enumerate(ldamodel[corpus]):\n",
        "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
        "        # print(row)\n",
        "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
        "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
        "        for j, (topic_num, prop_topic) in enumerate(row):\n",
        "            if j == 0:  # => dominant topic\n",
        "                wp = ldamodel.show_topic(topic_num)\n",
        "                topic_keywords = \", \".join([word for word, prop in wp])\n",
        "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
        "            else:\n",
        "                break\n",
        "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
        "\n",
        "    # Add original text to the end of the output\n",
        "    contents = pd.Series(texts)\n",
        "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
        "    return(sent_topics_df)\n",
        "\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
        "\n",
        "cloud = WordCloud(stopwords=stop_words,\n",
        "                  background_color='white',\n",
        "                  width=2500,\n",
        "                  height=1800,\n",
        "                  max_words=10,\n",
        "                  colormap='tab10',\n",
        "                  color_func=lambda *args, **kwargs: cols[i],\n",
        "                  prefer_horizontal=1.0)\n",
        "\n",
        "topics = lda_model.show_topics(formatted=False)\n",
        "\n",
        "fig, axes = plt.subplots(5,1, figsize=(15,15), sharex=True, sharey=True)\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    fig.add_subplot(ax)\n",
        "    topic_words = dict(topics[i][1])\n",
        "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
        "    plt.gca().imshow(cloud)\n",
        "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
        "    plt.gca().axis('off')\n",
        "\n",
        "\n",
        "plt.subplots_adjust(wspace=0, hspace=0)\n",
        "plt.axis('off')\n",
        "plt.margins(x=0, y=0)\n",
        "plt.tight_layout()\n",
        "plt.savefig('industry_lda_topic'+\".png\", bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=industry_lemmatized):\n",
        "    # Init output\n",
        "    sent_topics_df = pd.DataFrame()\n",
        "\n",
        "    # Get main topic in each document\n",
        "    for i, row_list in enumerate(ldamodel[corpus]):\n",
        "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
        "        # print(row)\n",
        "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
        "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
        "        for j, (topic_num, prop_topic) in enumerate(row):\n",
        "            if j == 0:  # => dominant topic\n",
        "                wp = ldamodel.show_topic(topic_num)\n",
        "                topic_keywords = \", \".join([word for word, prop in wp])\n",
        "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
        "            else:\n",
        "                break\n",
        "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
        "\n",
        "    # Add original text to the end of the output\n",
        "    contents = pd.Series(texts)\n",
        "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
        "    return(sent_topics_df)\n",
        "\n",
        "\n",
        "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=industry_lemmatized)\n",
        "\n",
        "# Format\n",
        "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
        "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
        "df_dominant_topic.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sentiment_analysis(inputfile, topic, brand):\n",
        "    if __name__ == \"__main__\":\n",
        "\n",
        "        Count_neg=0\n",
        "        Count_neutral=0\n",
        "        Count_pos=0\n",
        "        \n",
        "\n",
        "        for line in inputfile:\n",
        "            try:\n",
        "                print(line)\n",
        "                line = TextBlob(line)\n",
        "\n",
        "            \n",
        "                print (\"Input sentence = \", line.strip())            \n",
        "                print(line.sentiment)\n",
        "                \n",
        "\n",
        "                if line.sentiment.polarity < 0.0:\n",
        "                    print (\"negative\" )                \n",
        "                    Count_neg +=1\n",
        "                elif line.sentiment.polarity == 0:\n",
        "                    print (\"neutral\")                \n",
        "                    Count_neutral +=1\n",
        "                else:\n",
        "                    print (\"positive\")                \n",
        "                    Count_pos +=1\n",
        "                    \n",
        "                print(\"==============================================================\")\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        print (\"Count Negative = \", Count_neg)\n",
        "        print (\"Count Neutral = \", Count_neutral)\n",
        "        print (\"Count Positive = \", Count_pos)\n",
        "        \n",
        "\n",
        "        Polarity = [1,2,3]\n",
        "        LABELS = [\"Positive\", \"Neutral\", \"Negative\"]\n",
        "        Count_polarity = [Count_pos, Count_neutral, Count_neg]\n",
        "\n",
        "        plt.xlabel('Polarity')\n",
        "        plt.ylabel('Count')\n",
        "        plt.title('Sentiment Analysis Topic '+str(topic))\n",
        "\n",
        "        plt.grid(True)\n",
        "        plt.bar(Polarity, Count_polarity, align='center')\n",
        "        plt.xticks(Polarity, LABELS)\n",
        "        plt.savefig(brand+'_sentiment_topic_'+str(topic)+\".png\", bbox_inches='tight')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_0 = []\n",
        "topic_1 = []\n",
        "topic_2 = []\n",
        "topic_3 = []\n",
        "topic_4 = []\n",
        "\n",
        "\n",
        "for index,row in df_dominant_topic.iterrows():\n",
        "    if row[\"Dominant_Topic\"] == 0:\n",
        "        topic_0.append(row[\"Text\"])\n",
        "    elif row[\"Dominant_Topic\"] == 1:\n",
        "        topic_1.append(row[\"Text\"])\n",
        "    elif row[\"Dominant_Topic\"] == 2:\n",
        "        topic_2.append(row[\"Text\"])\n",
        "    elif row[\"Dominant_Topic\"] == 3:\n",
        "        topic_3.append(row[\"Text\"])\n",
        "    elif row[\"Dominant_Topic\"] == 4:\n",
        "        topic_4.append(row[\"Text\"])             \n",
        "        \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "\n",
        "############ topic 0\n",
        "temp = []\n",
        "for s in topic_0:\n",
        "    temp.append(TreebankWordDetokenizer().detokenize(s))\n",
        "   \n",
        "topic_0 = temp  \n",
        "with open('topic_0.txt', 'w',encoding=\"utf-8\") as f:\n",
        "    for item in topic_0:\n",
        "        f.write(\"%s\\n\" % item) \n",
        "\n",
        "######## topic 1        \n",
        "temp = []\n",
        "for s in topic_1:\n",
        "    temp.append(TreebankWordDetokenizer().detokenize(s))\n",
        "   \n",
        "topic_1 = temp  \n",
        "with open('topic_1.txt', 'w',encoding=\"utf-8\") as f:\n",
        "    for item in topic_1:\n",
        "        f.write(\"%s\\n\" % item) \n",
        "        \n",
        "\n",
        "########### topic 2\n",
        "temp = []\n",
        "for s in topic_2:\n",
        "    temp.append(TreebankWordDetokenizer().detokenize(s))\n",
        "   \n",
        "topic_2 = temp  \n",
        "with open('topic_2.txt', 'w',encoding=\"utf-8\") as f:\n",
        "    for item in topic_2:\n",
        "        f.write(\"%s\\n\" % item) \n",
        "\n",
        "\n",
        "########## topic 3       \n",
        "temp = []\n",
        "for s in topic_3:\n",
        "    temp.append(TreebankWordDetokenizer().detokenize(s))\n",
        "   \n",
        "topic_3 = temp  \n",
        "with open('topic_3.txt', 'w',encoding=\"utf-8\") as f:\n",
        "    for item in topic_3:\n",
        "        f.write(\"%s\\n\" % item) \n",
        "        \n",
        "        \n",
        "############## topic 4\n",
        "temp = []\n",
        "for s in topic_4:\n",
        "    temp.append(TreebankWordDetokenizer().detokenize(s))\n",
        "   \n",
        "topic_4 = temp  \n",
        "with open('topic_4.txt', 'w',encoding=\"utf-8\") as f:\n",
        "    for item in topic_4:\n",
        "        f.write(\"%s\\n\" % item) \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "         \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inputfile = open(\"topic_0.txt\", \"r\",encoding=\"utf-8\")\n",
        "sentiment_analysis(inputfile,0,\"industry\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inputfile = open(\"topic_1.txt\", \"r\",encoding=\"utf-8\")\n",
        "sentiment_analysis(inputfile,1,\"industry\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inputfile = open(\"topic_2.txt\", \"r\",encoding=\"utf-8\")\n",
        "sentiment_analysis(inputfile,2,\"industry\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inputfile = open(\"topic_3.txt\", \"r\",encoding=\"utf-8\")\n",
        "sentiment_analysis(inputfile,3,\"industry\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inputfile = open(\"topic_4.txt\", \"r\",encoding=\"utf-8\")\n",
        "sentiment_analysis(inputfile,4,\"industry\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMuLZ2IxrPmG",
        "outputId": "9811190b-0709-473b-c535-99094f2ad9db"
      },
      "outputs": [],
      "source": [
        "from gensim.models import TfidfModel\n",
        "#creating samsung lda model\n",
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(samsung_lemmatized)\n",
        "\n",
        "# Create Corpus\n",
        "texts = samsung_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "coherence = []\n",
        "for k in range(2,10):\n",
        "    print('Round: '+str(k))\n",
        "    Lda = gensim.models.ldamodel.LdaModel\n",
        "    ldamodel = Lda(corpus, num_topics=k, id2word = id2word, passes=20,\\\n",
        "                   iterations=20, chunksize = 100, eval_every = None, random_state=100)\n",
        "    \n",
        "    cm = gensim.models.coherencemodel.CoherenceModel(model=ldamodel, texts=texts,\\\n",
        "                                                     dictionary=id2word, coherence='c_v')\n",
        "    coherence.append((k,cm.get_coherence()))\n",
        "    \n",
        "x_val = [x[0] for x in coherence]\n",
        "y_val = [x[1] for x in coherence]\n",
        "\n",
        "\n",
        "plt.plot(x_val,y_val)\n",
        "plt.scatter(x_val,y_val)\n",
        "plt.title('Number of Topics vs. Coherence')\n",
        "plt.xlabel('Number of Topics')\n",
        "plt.ylabel('Coherence')\n",
        "plt.xticks(x_val)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=8, \n",
        "                                           chunksize=100,\n",
        "                                           passes=20,\n",
        "                                           iterations=20, \n",
        "                                           eval_every = None,\n",
        "                                           random_state=100\n",
        "                                           )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZBzuf9btrkG",
        "outputId": "036f7ba2-17ae-401e-bfc0-ad4965741e80"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "# Print the Keyword in the 10 topics\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "2pY8wSF1tw2E",
        "outputId": "a9009187-fe19-472b-d388-2d074303a190"
      },
      "outputs": [],
      "source": [
        "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=samsung_lemmatized):\n",
        "    # Init output\n",
        "    sent_topics_df = pd.DataFrame()\n",
        "\n",
        "    # Get main topic in each document\n",
        "    for i, row_list in enumerate(ldamodel[corpus]):\n",
        "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
        "        # print(row)\n",
        "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
        "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
        "        for j, (topic_num, prop_topic) in enumerate(row):\n",
        "            if j == 0:  # => dominant topic\n",
        "                wp = ldamodel.show_topic(topic_num)\n",
        "                topic_keywords = \", \".join([word for word, prop in wp])\n",
        "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
        "            else:\n",
        "                break\n",
        "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
        "\n",
        "    # Add original text to the end of the output\n",
        "    contents = pd.Series(texts)\n",
        "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
        "    return(sent_topics_df)\n",
        "\n",
        "\n",
        "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=samsung_lemmatized)\n",
        "\n",
        "# Format\n",
        "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
        "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
        "df_dominant_topic.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yXKhGIpfuL6E",
        "outputId": "05f00769-1172-4058-f66f-e2f357432b77"
      },
      "outputs": [],
      "source": [
        "# 1. Wordcloud of Top N words in each topic\n",
        "from matplotlib import pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.colors as mcolors\n",
        "stop_words = stopwords.words('english')\n",
        "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
        "\n",
        "cloud = WordCloud(stopwords=stop_words,\n",
        "                  background_color='white',\n",
        "                  width=2500,\n",
        "                  height=1800,\n",
        "                  max_words=10,\n",
        "                  colormap='tab10',\n",
        "                  color_func=lambda *args, **kwargs: cols[i],\n",
        "                  prefer_horizontal=1.0)\n",
        "\n",
        "topics = lda_model.show_topics(formatted=False)\n",
        "\n",
        "fig, axes = plt.subplots(4,2, figsize=(15,15), sharex=True, sharey=True)\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    fig.add_subplot(ax)\n",
        "    topic_words = dict(topics[i][1])\n",
        "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
        "    plt.gca().imshow(cloud)\n",
        "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
        "    plt.gca().axis('off')\n",
        "    \n",
        "\n",
        "\n",
        "plt.subplots_adjust(wspace=0, hspace=0)\n",
        "plt.axis('off')\n",
        "plt.margins(x=0, y=0)\n",
        "plt.tight_layout()\n",
        "plt.savefig('samsung_lda_topic'+\".png\", bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "samsung_topic_0 = []\n",
        "samsung_topic_1 = []\n",
        "samsung_topic_2 = []\n",
        "samsung_topic_3 = []\n",
        "samsung_topic_4 = []\n",
        "samsung_topic_5 = []\n",
        "samsung_topic_5 = []\n",
        "samsung_topic_6 = []\n",
        "samsung_topic_7 = []\n",
        "\n",
        "for index,row in df_dominant_topic.iterrows():\n",
        "    if row[\"Dominant_Topic\"] == 0:\n",
        "        samsung_topic_0.append(row[\"Text\"])\n",
        "    elif row[\"Dominant_Topic\"] == 1:\n",
        "        samsung_topic_1.append(row[\"Text\"])\n",
        "    elif row[\"Dominant_Topic\"] == 2:\n",
        "        samsung_topic_2.append(row[\"Text\"])\n",
        "    elif row[\"Dominant_Topic\"] == 3:\n",
        "        samsung_topic_3.append(row[\"Text\"])\n",
        "    elif row[\"Dominant_Topic\"] == 4:\n",
        "        samsung_topic_4.append(row[\"Text\"])\n",
        "    elif row[\"Dominant_Topic\"] == 5:\n",
        "        samsung_topic_5.append(row[\"Text\"])\n",
        "    elif row[\"Dominant_Topic\"] == 6:\n",
        "        samsung_topic_6.append(row[\"Text\"])  \n",
        "    elif row[\"Dominant_Topic\"] == 7:\n",
        "        samsung_topic_7.append(row[\"Text\"])   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "temp = []\n",
        "for s in samsung_topic_0:\n",
        "    temp.append(TreebankWordDetokenizer().detokenize(s))\n",
        "   \n",
        "samsung_topic_0 = temp  \n",
        "with open('samsung_topic_0.txt', 'w',encoding=\"utf-8\") as f:\n",
        "    for item in samsung_topic_0:\n",
        "        f.write(\"%s\\n\" % item) \n",
        "\n",
        "######## topic 1        \n",
        "temp = []\n",
        "for s in samsung_topic_1:\n",
        "    temp.append(TreebankWordDetokenizer().detokenize(s))\n",
        "   \n",
        "samsung_topic_1 = temp  \n",
        "with open('samsung_topic_1.txt', 'w',encoding=\"utf-8\") as f:\n",
        "    for item in samsung_topic_1:\n",
        "        f.write(\"%s\\n\" % item) \n",
        "        \n",
        "\n",
        "########### topic 2\n",
        "temp = []\n",
        "for s in samsung_topic_2:\n",
        "    temp.append(TreebankWordDetokenizer().detokenize(s))\n",
        "   \n",
        "samsung_topic_2 = temp  \n",
        "with open('samsung_topic_2.txt', 'w',encoding=\"utf-8\") as f:\n",
        "    for item in samsung_topic_2:\n",
        "        f.write(\"%s\\n\" % item) \n",
        "\n",
        "\n",
        "########## topic 3       \n",
        "temp = []\n",
        "for s in samsung_topic_3:\n",
        "    temp.append(TreebankWordDetokenizer().detokenize(s))\n",
        "   \n",
        "samsung_topic_3 = temp  \n",
        "with open('samsung_topic_3.txt', 'w',encoding=\"utf-8\") as f:\n",
        "    for item in samsung_topic_3:\n",
        "        f.write(\"%s\\n\" % item) \n",
        "        \n",
        "        \n",
        "############## topic 4\n",
        "temp = []\n",
        "for s in samsung_topic_4:\n",
        "    temp.append(TreebankWordDetokenizer().detokenize(s))\n",
        "   \n",
        "samsung_topic_4 = temp  \n",
        "with open('samsung_topic_4.txt', 'w',encoding=\"utf-8\") as f:\n",
        "    for item in samsung_topic_4:\n",
        "        f.write(\"%s\\n\" % item) \n",
        "        \n",
        "        \n",
        "        \n",
        "####################topic 5\n",
        "temp = []\n",
        "for s in samsung_topic_5:\n",
        "    temp.append(TreebankWordDetokenizer().detokenize(s))\n",
        "   \n",
        "samsung_topic_5 = temp  \n",
        "with open('samsung_topic_5.txt', 'w',encoding=\"utf-8\") as f:\n",
        "    for item in samsung_topic_5:\n",
        "        f.write(\"%s\\n\" % item) \n",
        "        \n",
        "####################topic 6\n",
        "temp = []\n",
        "for s in samsung_topic_6:\n",
        "    temp.append(TreebankWordDetokenizer().detokenize(s))\n",
        "   \n",
        "samsung_topic_6 = temp  \n",
        "with open('samsung_topic_6.txt', 'w',encoding=\"utf-8\") as f:\n",
        "    for item in samsung_topic_6:\n",
        "        f.write(\"%s\\n\" % item) \n",
        "        \n",
        " ####################topic 7\n",
        "temp = []\n",
        "for s in samsung_topic_7:\n",
        "    temp.append(TreebankWordDetokenizer().detokenize(s))\n",
        "   \n",
        "samsung_topic_7 = temp  \n",
        "with open('samsung_topic_7.txt', 'w',encoding=\"utf-8\") as f:\n",
        "    for item in samsung_topic_7:\n",
        "        f.write(\"%s\\n\" % item)        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inputfile = open(\"samsung_topic_0.txt\", \"r\",encoding=\"utf-8\")\n",
        "sentiment_analysis(inputfile,0,\"samsung\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inputfile = open(\"samsung_topic_1.txt\", \"r\",encoding=\"utf-8\")\n",
        "sentiment_analysis(inputfile,1,\"samsung\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inputfile = open(\"samsung_topic_2.txt\", \"r\",encoding=\"utf-8\")\n",
        "sentiment_analysis(inputfile,2,\"samsung\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inputfile = open(\"samsung_topic_3.txt\", \"r\",encoding=\"utf-8\")\n",
        "sentiment_analysis(inputfile,3,\"samsung\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inputfile = open(\"samsung_topic_4.txt\", \"r\",encoding=\"utf-8\")\n",
        "sentiment_analysis(inputfile,4,\"samsung\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inputfile = open(\"samsung_topic_5.txt\", \"r\",encoding=\"utf-8\")\n",
        "sentiment_analysis(inputfile,5,\"samsung\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inputfile = open(\"samsung_topic_6.txt\", \"r\",encoding=\"utf-8\")\n",
        "sentiment_analysis(inputfile,6,\"samsung\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inputfile = open(\"samsung_topic_7.txt\", \"r\",encoding=\"utf-8\")\n",
        "sentiment_analysis(inputfile,7,\"samsung\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of Untitled6.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "6d46af94c2bbce495f1e668725902fa517c90b1782bcfe2fce0dd9868df553d3"
    },
    "kernelspec": {
      "display_name": "Python 3.7.7 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
